@article{devries_learning_2018,
 abstract = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.},
 author = {DeVries, Terrance and Taylor, Graham W.},
 copyright = {All rights reserved},
 file = {arXiv\:1802.04865 PDF:C\:\\Users\\Terrance\\Zotero\\storage\\T89PL3P3\\DeVries and Taylor - 2018 - Learning Confidence for Out-of-Distribution Detect.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Terrance\\Zotero\\storage\\R95PGVGK\\1802.html:text/html},
 journal = {arXiv:1802.04865 [cs, stat]},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 month = {feb},
 note = {arXiv: 1802.04865},
 title = {Learning Confidence for Out-of-Distribution Detection in Neural Networks},
 url = {http://arxiv.org/abs/1802.04865},
 urldate = {2019-01-22},
 year = {2018}
}

